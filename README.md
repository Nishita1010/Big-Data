
# Newspaper Data Analysis

The newspaper data analysis project analyzed data from 20 newspapers, totaling 20,000 folders, to understand reader preferences and behavior. It used data mining, natural language processing, and machine learning to uncover insights, leading to personalized content recommendations and improved content strategies, ultimately boosting reader engagement. This project showcased how big data can transform traditional media and drive innovation in the news industry.


## Motivation behind the project:

- *Demonstrate Practical Big Data Usage:* Show how big data applies in real-world situations, specifically in the newspaper industry.

- *Develop Hands-On Expertise:* Gain practical experience in cleaning, processing, and analyzing data using big data tools.

## Technical Features and Elements:
- Data Ingestion
- Hadoop Distributed File System (HDFS)
- Hadoop MapReduce
- Apache Hive
- Apache Spark
- Apache Sqoop
- Machine Learning Algorithms (e.g., Logistic Regression)
- Data Visualization (e.g., Tableau)
- Cloud Computing (Code Spaces by GitHub)


## The problems addressed:
- *Dealing with Massive Data Volumes:* Tackling the challenge of managing, processing, and analyzing extensive data sets can empower the industry to extract valuable insights. Furthermore, having abundant data is an advantage and can mitigate the risk of overfitting.

- *Tailoring Content to Individual Preferences:* Customizing content based on individual customer preferences can enable the industry to enhance its revenue streams and attract a broader readership.

## Methodology

The methodology followed while building the project was as follows:

• *Data Collection / Dataset:* The initial step in any data analysis project involves data collection. For this project, I utilized the "20 Newsgroups data set," consisting of around 20,000 newsgroup documents evenly distributed across 20 different newsgroups. This dataset, originally compiled by Ken Lang, was in an unstructured format.

• *Data Cleaning:* I eliminated unnecessary characters and null values from the dataset. Additionally, I associated news types with average word counts to determine the word count required for news type detection. I formatted the data for ease of processing by Hadoop, Hive, and Spark frameworks.

• *Data Processing:* The cleaned data underwent processing using the Hadoop MapReduce framework, enabling distributed processing of large datasets. This involved breaking down the data into smaller portions, distributing them across a cluster of computers, and parallel processing. I also implemented a deep learning model using logistic regression and the sigmoid function to enable the model to learn and recognize patterns in the data.

• *Data Analysis:* The processed data was then loaded into Hive, a data warehousing and SQL-like query language for Hadoop. Logistic regression was applied to the data for extracting insights and identifying patterns.

• *Data Visualization:* Finally, the analyzed data was visualized using various tools and techniques, including charts.













